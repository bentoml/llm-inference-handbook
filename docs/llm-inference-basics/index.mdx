---
sidebar_position: 1
sidebar_custom_props: 
    icon: /img/book.svg
    collapsed: false
---

# LLM inference basics

LLM inference is where models meet the real world. It powers everything from instant chat replies to code generation, and directly impacts latency, cost, and user experience. Understanding how inference works is the first step toward building smarter, faster, and more reliable AI applications.

```mdx-code-block
import DocCardList from '@theme/DocCardList';

<DocCardList />
```