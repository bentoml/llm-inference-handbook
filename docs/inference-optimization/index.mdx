---
sidebar_position: 4
sidebar_custom_props: 
    icon: /img/speed.svg
---

# Inference optimization

Running an LLM is just the starting point. Making it fast, efficient, and scalable is where inference optimization comes into play. Whether you're building a chatbot, an agent, or any LLM-powered tool, inference performance directly impacts both user experience and operational cost. 

If you're using a serverless endpoint (e.g., OpenAI API), much of this work is abstracted away. But if you're self-hosting open-source or custom models, applying the right optimization techniques lets you adapt to different use cases. This is how you can build faster, smarter, and more cost-effective AI applications than your competitors.

```mdx-code-block
import DocCardList from '@theme/DocCardList';

<DocCardList />
```