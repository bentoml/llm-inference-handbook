# ğŸ“– LLM Inference in Production

This repository contains the source content for [LLM Inference in Production](https://bentoml.com/llm/), a practical guide for understanding, optimizing, scaling, and operating LLM inference.

[![Twitter](https://badgen.net/badge/icon/@bentomlai/1DA1F2?icon=twitter&label=Follow)](https://twitter.com/bentomlai)
[![Community](https://badgen.net/badge/Join/Community/cyan?icon=slack)](https://l.bentoml.com/join-slack)

## ğŸ”§ Local preview

To preview the site locally:

```bash
pnpm install
pnpm start
```

It will be running at [http://localhost:3000/llm/](http://localhost:3000/llm/).

## ğŸ¤ Contributing

Contributions are welcome! Feel free to open issues, suggest improvements, or submit pull requests.

## ğŸ“„ License

This project is licensed under the [MIT License](https://github.com/bentoml/llm-inference-in-production/blob/main/LICENSE).
